{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d396a8",
   "metadata": {},
   "source": [
    "# 1.) Project's each line of code contains an explanation.\n",
    "\n",
    "# 2.) The explanation for each line of the code will ease the task of working on the project and understanding it.\n",
    "\n",
    "Author: Abhyas Mall (Research Intern - CAIR - DRDO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f268917",
   "metadata": {},
   "source": [
    "Data set features->\n",
    "\n",
    "1.)id: Unique id for a news article.\n",
    "\n",
    "2.)title: Title of news article.\n",
    "\n",
    "3.)author: Author of the news article.\n",
    "\n",
    "4.)text: Text of the article; might be incomplete.\n",
    "\n",
    "5.)label: A label that marks whether the news article is genuine/fake (ie:1/0), 1 for fake and 0 for genuine article.\n",
    "\n",
    "Training and testing dataset: https://www.kaggle.com/c/fake-news/data?select=train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce88f7f",
   "metadata": {},
   "source": [
    "# Importing the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fab1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#numpy is very useful for making numpy-arrays\n",
    "import pandas as pd\n",
    "#helpful in creating data frame and storing data into it\n",
    "import re #importing regular expression-useful for searching text in a paragraph.\n",
    "\n",
    "#USING OF THE NATURAL LANGUAGE TOOLKIT\n",
    "#STEPS INVOLVING IN NATURAL LANGUAGE PROCESSING-\n",
    "\n",
    "#STEP-1: Tokenizing is not required-\n",
    "#tokenization will be done using \".split()\" during the stemming process\n",
    "\n",
    "#STEP2: for stopwords elimination we import stopwords\n",
    "from nltk.corpus import stopwords \n",
    "#nltk will help analyzing, pull apart text, help using of stopwords, tag things such as part of speech named to ND, etc.\n",
    "#stopwords are of no use to the computing system, eg: a, the, etc.\n",
    "#using of stopwords will save the processing time.\n",
    "#corpus- is basically the important content of the text\n",
    "#nltk- natural language toolkit\n",
    "\n",
    "#STEP3: For stemming:\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "#stemming takes a word, removes its prefix and suffix, and returns the root word of it.\n",
    "#NLTK- is a toolkit \n",
    "\n",
    "#STEP4: To identify significant/important words in a document\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#they are imported to convert text into feature vectors.\n",
    "#TfidfVectorizer's main role is to find the most important/significant words from the document\n",
    "#tf-term(words) frequency\n",
    "#idf- inverse document frequency\n",
    "#TF(t)=freq. of the term in a doc/total no. of words in a doc\n",
    "#IDF(t)=log(Total no. of documents)/no. of documents with term t in it\n",
    "#Tfidf=tf*idf\n",
    "#if Tdidf is higher it means the word is more significant.\n",
    "\n",
    "#STEP5: To split the data-set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "#they are imported to split data-set into training and testing data\n",
    "\n",
    "#STEP6:To import the ML model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#they are imported to use the Logistic Regression model\n",
    "\n",
    "#STEP 7:For evaluation of the model.\n",
    "from sklearn.metrics import accuracy_score\n",
    "#to evaluate the performance of the model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "952cb422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mallabhyas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first we need to download stopwords from nltk library\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95ba18f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#to check-out if the downloaded stopwords are sufficient or we need to add\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c90b5e",
   "metadata": {},
   "source": [
    "# Data Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ee97cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset into a pandas dataframe\n",
    "news_dataset=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f522da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.head()\n",
    "#to take an overview of the dataframe\n",
    "#label 1 signifies fake news\n",
    "#label 0 signifies genuine news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8a86ebdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.shape\n",
    "#to know the no. of articles (no. of rows and columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7aa9ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to check if values are missing, or no. of missing values in the dataset\n",
    "news_dataset.isnull().sum()\n",
    "#this will count the number of missing values in \"EACH COLUMN\".\n",
    "#While preparing dataset we might not get the author/title\n",
    "#But label has no null value therefore that is not a problem.\n",
    "#the missing values are insignificant in number, when compared to the size of the data set\n",
    "#if missing values were more in number then we would have used methods such as imputation.\n",
    "#imputation: processing, that's gonna replace the missing values with appropriate values.\n",
    "#hence, imputation isn't required in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e645a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing the null values with empty string\n",
    "news_dataset=news_dataset.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "27d2ccdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "title     0\n",
       "author    0\n",
       "text      0\n",
       "label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "11818ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for our prediction we are gonna use the column title and author\n",
    "#hence, we will combine title and author\n",
    "#we will not use the text because mostly they are huge paragraphs,\n",
    "#will take huge computation power, also won't do much good to our model.\n",
    "\n",
    "#if the accuracy is not satisfactory then we might use texts as well.\n",
    "\n",
    "#merging the author name and news title in content column\n",
    "\n",
    "news_dataset['content']=news_dataset['author']+' '+news_dataset['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c09820ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Darrell Lucus House Dem Aide: We Didn’t Even S...\n",
      "1        Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...\n",
      "2        Consortiumnews.com Why the Truth Might Get You...\n",
      "3        Jessica Purkiss 15 Civilians Killed In Single ...\n",
      "4        Howard Portnoy Iranian woman jailed for fictio...\n",
      "                               ...                        \n",
      "20795    Jerome Hudson Rapper T.I.: Trump a ’Poster Chi...\n",
      "20796    Benjamin Hoffman N.F.L. Playoffs: Schedule, Ma...\n",
      "20797    Michael J. de la Merced and Rachel Abrams Macy...\n",
      "20798    Alex Ansary NATO, Russia To Hold Parallel Exer...\n",
      "20799              David Swanson What Keeps the F-35 Alive\n",
      "Name: content, Length: 20800, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(news_dataset['content'])\n",
    "#this column will be used to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac4e86a",
   "metadata": {},
   "source": [
    "# Stemming:\n",
    "Stemming is the process of NLP in which we reduce the words to its Root Word.\n",
    "eg:actor,actress,acting --> root word is \"act\"\n",
    "so basically the prefixes and suffixes will be removed.\n",
    "\n",
    "We gotta reduce words for performance enhancement->\n",
    "after stemming we'll perform #vectorizing. \n",
    "#in vectorizing we will convert the words into feature vectors-or, numeric data.\n",
    "\n",
    "text data->numeric data->model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "863c3240",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem=PorterStemmer()\n",
    "#function is stored in port_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a5e64144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are creating a function stemming\n",
    "def stemming(content):\n",
    "    stemmed_content=re.sub('[^a-zA-Z]',' ',content)\n",
    "    #regular expression library uses sub which substitutes\n",
    "    #certain values '^'-means exclusion\n",
    "    #it means we are mentioning a set which contains a-z and A-Z\n",
    "    #in our data set we don't want numbers and (,.)quotation marks etc\n",
    "    #all we want is text data- so we only take data in form of a-z or A-Z\n",
    "    #excluding everything other than a-z and A-Z\n",
    "    #' 'space means if there is anything other than a-z A-Z it will get replaced by a blank space\n",
    "    \n",
    "    stemmed_content=stemmed_content.lower()\n",
    "    #here we convert all the alphabets into lower case\n",
    "    #because ML model will consider upper case words and lower case words differently even if they mean the same\n",
    "    #so that the processing is done smoothly\n",
    "    \n",
    "    stemmed_content=stemmed_content.split()\n",
    "    #tokenizing- basically the words will be stored in a list.\n",
    "    \n",
    "    stemmed_content=[port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    #we are stemming each word using port_stem(function) except the stopwords\n",
    "    \n",
    "    stemmed_content=' '.join(stemmed_content)\n",
    "    #we will join all the words later on into a sentences\n",
    "    \n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "92563888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daniel j flynn flynn hillari clinton big woman campu breitbart'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking out the stemming function\n",
    "stemming(news_dataset.loc[1,'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cf77c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset['content']=news_dataset['content'].apply(stemming)\n",
    "#each row in the content column will be stemmed and stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6033767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_dataset['content'])\n",
    "#no upper case letters\n",
    "#no stopwords\n",
    "#only root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bac276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the data and label\n",
    "X=news_dataset['content'].values\n",
    "Y=news_dataset['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45146df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fb173",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc1d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6680c6c",
   "metadata": {},
   "source": [
    "# Converting the textual data into numeric data (feature vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer()\n",
    "vectorizer.fit(X)\n",
    "X=vectorizer.transform(X)\n",
    "\n",
    "#tf-term frequency\n",
    "#counts no. of times a word is repeating in a doc/paragraph\n",
    "#and assigns a numeric value. the greater the more significant\n",
    "\n",
    "#idf-inverse document frequency\n",
    "#sometimes words that is repeated several times and might not have significance\n",
    "#eg: building a system that predicts if review is positive or negative\n",
    "#eg: we are analysing all reviews for movie avengers\n",
    "#so the word avengers will obviously be repeated again and again\n",
    "#and it also does not have much significance\n",
    "#so the idf basically reduces its numeric/importance value\n",
    "\n",
    "#Finally the Feature vector is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38409baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728d7d1",
   "metadata": {},
   "source": [
    "# SPLITTING DATASET TO TRAINING AND TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed578db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify=Y, random_state=2)\n",
    "#80% of the data will be used for training\n",
    "#20% of the data will be used for testing (0.2)\n",
    "#Stratify=y is used so that the genuine and the fake news data is in equal proprtion as it was in original dataset\n",
    "#Random test is basically used for reproducing the same set of train and test split.\n",
    "#If random test isn't used then every time we run we might get a different set of training and testing data.\n",
    "#can be any integer value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37309398",
   "metadata": {},
   "source": [
    "# Training the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd921d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f6c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)\n",
    "#The Sigmoid function curve will be plotted using logistic regression for the above training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6640b49",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy score on the training data\n",
    "X_train_prediction = model.predict(X_train)\n",
    "#model will predict X_train dataset and the label predicted by the model will be stored in the above variable.\n",
    "training_data_accuracy = accuracy_score(X_train_prediction, Y_train)\n",
    "#accuracy score will be calculated by putting predicted labels and original labels into the function.\n",
    "#it is compared and the accuracy score is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6433255",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TRAINING DATA ACCURACY SCORE {training_data_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf5f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_train,X_train_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb6dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score on the test data\n",
    "X_test_prediction = model.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(X_test_prediction, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d9cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'TEST DATA ACCURACY {test_data_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test,X_test_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f15ffb",
   "metadata": {},
   "source": [
    "# Making a predictive system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c747ef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[1]\n",
    "\n",
    "prediction=model.predict(X_new)\n",
    "if(prediction[0]==0):\n",
    "    print(\"The news is Real\")\n",
    "else:\n",
    "    print(\"The news is Fake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec9f06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
